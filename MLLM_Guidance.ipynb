{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dTVsJrpcNQMd"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import base64\n",
        "import requests\n",
        "import json\n",
        "import ast\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=userdata.get('OPENAI_API_KEY'),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x54vlqtXljNA"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade diffusers\n",
        "\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(\n",
        "    \"jiuntian/gligen-xl-1024\", trust_remote_code=True, torch_dtype=torch.float16\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55MzyshmDN5h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def get_rect(bbox):\n",
        "  elements = bbox.split('[')\n",
        "  object_name = elements[0].strip()\n",
        "  coordinates = elements[1].split(']')[0].split(', ')\n",
        "  bbox_coor = []\n",
        "  for i in coordinates:\n",
        "    bbox_coor.append(eval(i))\n",
        "\n",
        "  bbox_coor = torch.Tensor(bbox_coor)\n",
        "\n",
        "  x, y, w, h = bbox_coor[0], bbox_coor[1], bbox_coor[2], bbox_coor[3]\n",
        "  rect = [float(x), float(y), float(w), float(h)]\n",
        "  return object_name, rect\n",
        "\n",
        "def draw_layout(bboxes_list, path):\n",
        "\n",
        "  w = 512\n",
        "  h = 512\n",
        "\n",
        "  fnt = ImageFont.truetype(\"/usr/share/fonts/truetype/humor-sans/Humor-Sans.ttf\", 12)\n",
        "\n",
        "  img = Image.new(\"RGB\", (w, h), 'white')\n",
        "  img1 = ImageDraw.Draw(img)\n",
        "\n",
        "  bboxes = []\n",
        "  for el in bboxes_list:\n",
        "    start = '['\n",
        "    end = ']'\n",
        "    s = el\n",
        "    obj = (s.split(start))[0].strip()\n",
        "    coor = (s.split(start))[1].split(end)[0]\n",
        "    coor_list = coor.split(', ')\n",
        "    temp_bboxes = []\n",
        "    for i in coor_list:\n",
        "      temp_bboxes.append(round(eval(i) * 512, 3))\n",
        "\n",
        "    el = obj + str(temp_bboxes)\n",
        "    bboxes.append(el)\n",
        "\n",
        "  color = \"black\"\n",
        "  for el in bboxes:\n",
        "    obj_name, obj_rect = get_rect(el)\n",
        "    img1.rectangle(obj_rect, outline = color)\n",
        "    img1.text((obj_rect[0], obj_rect[1]), obj_name, color, font=fnt)\n",
        "\n",
        "  layout_path = path\n",
        "  img.save(layout_path)\n",
        "  return layout_path\n",
        "\n",
        "def blank_layout(path):\n",
        "  w = 512\n",
        "  h = 512\n",
        "\n",
        "  img = Image.new(\"RGB\", (w, h), 'white')\n",
        "  layout_path = path\n",
        "  img.save(layout_path)\n",
        "  return layout_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikE7IAVzuwcu"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/MLLM-Guidance/LLM-prompts/grounding_prompt.txt\", \"r\") as my_file:\n",
        "  generator_prompt = my_file.read()\n",
        "\n",
        "with open(\"/content/MLLM-Guidance/LLM-prompts/self-correcting_prompt.txt\", \"r\") as my_file:\n",
        "  adjuster_prompt = my_file.read()\n",
        "\n",
        "with open(\"/content/MLLM-Guidance/LLM-prompts/reasoning_prompt.txt\", \"r\") as my_file:\n",
        "  evaluator_prompt = my_file.read()\n",
        "\n",
        "with open(\"/content/MLLM-Guidance/LLM-prompts/aligning_prompt.txt\", \"r\") as my_file:\n",
        "  aligner_prompt = my_file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVVKwIC3wSwo"
      },
      "outputs": [],
      "source": [
        "def generate_layout(caption, sys_prompt=generator_prompt):\n",
        "  prompt = sys_prompt + caption\n",
        "  start = 'Objects: '\n",
        "  gpt_exp = ''\n",
        "  while (gpt_exp == None) or (start not in gpt_exp):\n",
        "    response_1 = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "              {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "          }\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "      )\n",
        "\n",
        "    gpt_exp = response_1.choices[0].message.content\n",
        "    #print(gpt_exp)\n",
        "\n",
        "  s = gpt_exp\n",
        "  coordinates = s.split(start)[1].strip()\n",
        "\n",
        "  if coordinates.endswith(']\\']') or coordinates.endswith(']]') or coordinates.endswith('\\n]'):\n",
        "    coordinates = ast.literal_eval(coordinates)\n",
        "\n",
        "  return coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MK4V7JswhMW"
      },
      "outputs": [],
      "source": [
        "def adjust_layout(caption, bboxes, sys_prompt=adjuster_prompt):\n",
        "  post_prompt = 'User prompt: ' + caption + '\\nCurrent Objects: ' + str(bboxes)\n",
        "  prompt = sys_prompt + post_prompt\n",
        "  start = 'Updated Objects: '\n",
        "  gpt_exp = ''\n",
        "  while (gpt_exp == None) or (start not in gpt_exp):\n",
        "    response_2 = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "              {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "          }\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "      )\n",
        "\n",
        "    gpt_exp = response_2.choices[0].message.content\n",
        "    #print(gpt_exp)\n",
        "\n",
        "  s = gpt_exp\n",
        "  updated_coordinates = s.split(start)[1].strip()\n",
        "\n",
        "  if updated_coordinates.endswith(']\\']') or updated_coordinates.endswith(']]') or updated_coordinates.endswith('\\n]'):\n",
        "    updated_coordinates = ast.literal_eval(updated_coordinates)\n",
        "\n",
        "  return updated_coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBSnyEX70dYn"
      },
      "outputs": [],
      "source": [
        "def evaluate_image(caption, image_path, sys_prompt=evaluator_prompt):\n",
        "  # Function to encode the image\n",
        "  def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "      return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "  # Getting the base64 string\n",
        "  base64_image = encode_image(image_path)\n",
        "  prompt = sys_prompt + caption\n",
        "  start = 'Score: '\n",
        "  gpt_exp = ''\n",
        "  while (gpt_exp == None) or (start not in gpt_exp):\n",
        "    response_1 = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "              {\"type\": \"text\", \"text\": prompt},\n",
        "              {\n",
        "              \"type\": \"image_url\",\n",
        "              \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
        "              },\n",
        "            ],\n",
        "          }\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "      )\n",
        "\n",
        "    gpt_exp = response_1.choices[0].message.content\n",
        "    print(gpt_exp)\n",
        "\n",
        "  s = gpt_exp\n",
        "  score = s.split(start)[1].strip()\n",
        "\n",
        "  return int(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFKwe02xxhmm"
      },
      "outputs": [],
      "source": [
        "def align_image(caption, image_path, sys_prompt=aligner_prompt):\n",
        "  # Function to encode the image\n",
        "  def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "      return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "  # Getting the base64 string\n",
        "  base64_image = encode_image(image_path)\n",
        "\n",
        "  post_prompt = 'User prompt: ' + caption\n",
        "  prompt = sys_prompt + post_prompt\n",
        "  start = 'Updated Objects: '\n",
        "  gpt_exp = ''\n",
        "  while (gpt_exp == None) or (start not in gpt_exp):\n",
        "    response_3 = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "              {\"type\": \"text\", \"text\": prompt},\n",
        "              {\n",
        "              \"type\": \"image_url\",\n",
        "              \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
        "              },\n",
        "            ],\n",
        "          }\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "      )\n",
        "\n",
        "    gpt_exp = response_3.choices[0].message.content\n",
        "    #print(gpt_exp)\n",
        "\n",
        "  s = gpt_exp\n",
        "  updated_coordinates = s.split(start)[1].strip()\n",
        "\n",
        "  if updated_coordinates.endswith(']\\']') or updated_coordinates.endswith(']]') or updated_coordinates.endswith('\\n]'):\n",
        "    updated_coordinates = ast.literal_eval(updated_coordinates)\n",
        "\n",
        "  return updated_coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BIXfmLMzcin"
      },
      "outputs": [],
      "source": [
        "def generate_gligen_image(caption, bboxes, latents, pipe=pipeline):\n",
        "  gligen_boxes = []\n",
        "  gligen_phrases = []\n",
        "  for el in bboxes:\n",
        "    if el[0] == '[':\n",
        "      el = ast.literal_eval(el)[0]\n",
        "    start = '['\n",
        "    end = ']'\n",
        "    s = el\n",
        "    gligen_phrases.append((s.split(start))[0].strip())\n",
        "    coor = (s.split(start))[1].split(end)[0]\n",
        "    coor_list = coor.split(', ')\n",
        "    temp_bboxes = []\n",
        "    for i in coor_list:\n",
        "      temp_bboxes.append(eval(i))\n",
        "    gligen_boxes.append(temp_bboxes)\n",
        "\n",
        "  image = pipe(\n",
        "    prompt,\n",
        "    num_inference_steps=50,\n",
        "    height=1024, width=1024,\n",
        "    gligen_scheduled_sampling_beta=0.4,\n",
        "    gligen_boxes=gligen_boxes,\n",
        "    gligen_phrases=gligen_phrases,\n",
        "    num_images_per_prompt=1,\n",
        "  ).images[0]\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xq6w5b6a9cH"
      },
      "outputs": [],
      "source": [
        "def generate_latents(seed, pipe = pipeline):\n",
        "  #set manual latents\n",
        "  device = \"cuda\"\n",
        "  generator = torch.Generator(device=device)\n",
        "\n",
        "  latents = None\n",
        "  height, width = 1024, 1024\n",
        "  generator = generator.manual_seed(seed)\n",
        "  image_latents = torch.randn(\n",
        "          (1, pipe.unet.in_channels, height // 8, width // 8),\n",
        "          generator = generator,\n",
        "          device = device\n",
        "      )\n",
        "  latents = image_latents if latents is None else torch.cat((latents, image_latents))\n",
        "  return latents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL6hsfn5byw4"
      },
      "outputs": [],
      "source": [
        "def generate_grounded(caption, latents):\n",
        "  #print(\"Grounded layout:\")\n",
        "  bboxes = generate_layout(caption)\n",
        "  grounded_layout_path = draw_layout(bboxes, \"grounded_layout.jpg\")\n",
        "  image = generate_gligen_image(caption, bboxes, latents)\n",
        "  grounded_image_path = \"grounded_image.jpg\"\n",
        "  image.save(grounded_image_path)\n",
        "  return bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs1HJXDUcTlb"
      },
      "outputs": [],
      "source": [
        "def generate_adjusted(caption, latents, bboxes, cycle):\n",
        "  #print(\"Adjusted layout:\")\n",
        "  existing_bboxes = bboxes\n",
        "  i=0\n",
        "  while i < cycle:\n",
        "    adjusted_bboxes = adjust_layout(caption, existing_bboxes)\n",
        "    if adjusted_bboxes == existing_bboxes:\n",
        "      break\n",
        "    existing_bboxes = adjusted_bboxes\n",
        "    i=i+1\n",
        "  adjusted_layout_path = draw_layout(adjusted_bboxes, \"adjusted_layout.jpg\")\n",
        "  image = generate_gligen_image(caption, adjusted_bboxes, latents)\n",
        "  adjusted_image_path = \"adjusted_image.jpg\"\n",
        "  image.save(adjusted_image_path)\n",
        "  return adjusted_image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61HfJW1QHrHF"
      },
      "outputs": [],
      "source": [
        "def generate_aligned(caption, latents, image_path, cycle):\n",
        "  if 'grounded' in image_path:\n",
        "    image_name = \"aligned_grounded_image.jpg\"\n",
        "  elif 'adjusted' in image_path:\n",
        "    image_name = \"aligned_adjusted_image.jpg\"\n",
        "    layout_name = \"aligned_adjusted_layout.jpg\"\n",
        "  else:\n",
        "    image_name = \"aligned_image.jpg\"\n",
        "\n",
        "  existing_image_path = image_path\n",
        "  existing_score = 0\n",
        "\n",
        "  i=0\n",
        "  while existing_score != 5 and i < cycle:\n",
        "    aligned_bboxes = align_image(caption, existing_image_path)\n",
        "    if aligned_bboxes in ['','[]',' ', '[ ]']:\n",
        "      image = Image.open(existing_image_path)\n",
        "      image_path = image_name\n",
        "      image.save(image_path)\n",
        "      break\n",
        "    image = generate_gligen_image(caption, aligned_bboxes, latents)\n",
        "    temp_image_path = \"temp_image.jpg\"\n",
        "    image.save(temp_image_path)\n",
        "    score = evaluate_image(caption, temp_image_path)\n",
        "\n",
        "    if score >= existing_score:\n",
        "      aligned_adjusted_layout_path = draw_layout(aligned_bboxes, layout_name)\n",
        "      image_path = image_name\n",
        "      image.save(image_path)\n",
        "      existing_score = score\n",
        "      existing_image_path = image_path\n",
        "\n",
        "    i=i+1\n",
        "  return image_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC7tpu7LAJYL"
      },
      "outputs": [],
      "source": [
        "def text_to_image(seed, caption, cycle):\n",
        "  latents = generate_latents(seed, pipe = pipeline)\n",
        "\n",
        "  # Base image\n",
        "  # generate_base(caption, latents)\n",
        "\n",
        "  # Grounded image\n",
        "  bboxes = generate_grounded(caption, latents)\n",
        "\n",
        "  # Adjusting grounding information\n",
        "  adjusted_image_path = generate_adjusted(caption, latents, bboxes, cycle)\n",
        "\n",
        "  # Aligning grounded image\n",
        "  #grounded_image_path = \"grounded_image.jpg\"\n",
        "  #grounded_layout_path = \"grounded_layout.jpg\"\n",
        "  #aligned_grounded_bboxes = generate_aligned(caption, latents, grounded_image_path, cycle)\n",
        "\n",
        "  # Aligning adjusted image\n",
        "  aligned_adjusted_image_path = generate_aligned(caption, latents, adjusted_image_path, cycle)\n",
        "\n",
        "  return aligned_adjusted_image_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsRtizPQtq81"
      },
      "outputs": [],
      "source": [
        "def bench_evaluate(caption, image_path):\n",
        "  # Function to encode the image\n",
        "  def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "      return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "  # Getting the base64 string\n",
        "  base64_image = encode_image(image_path)\n",
        "\n",
        "  response_1 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\"type\": \"text\", \"text\": \"You are my assistant to evaluate the correspondence of the image to a given text prompt. Briefly describe the image within 50 words, focus on the objects in the image and their attributes (such as color, shape, texture), spatial layout and action relationships.\"},\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "            }\n",
        "          },\n",
        "        ],\n",
        "      }\n",
        "    ],\n",
        "    max_tokens=300,\n",
        "  )\n",
        "\n",
        "  gpt_exp = response_1.choices[0].message.content\n",
        "\n",
        "  input_text = \"According to the image and your previous answer, evaluate how well the image aligns with the text prompt: \" + caption + \". Give a score from 0 to 100, according to the criteria: \\n 100: the image is quite realistic and perfectly matches the content of the text prompt, with no discrepancies. \\n 75: the image is realistic and contains all objects but the relationships and actions are not complete. \\n 50: the image contains all objects but may be not realistic. \\n 25: the image contains the main object but not other objects that should be there. \\n 0: the image contains no objects given in the text prompt. Provide your analysis and explanation in JSON format with the following keys: score (e.g., 85), explanation (within 20 words).\"\n",
        "  response_2 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\"type\": \"text\", \"text\": input_text},\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "            }\n",
        "          },\n",
        "        ],\n",
        "      }\n",
        "    ],\n",
        "    max_tokens=300,\n",
        "  )\n",
        "\n",
        "  response_json = response_2.choices[0].message.content\n",
        "  start = '\\\"score\\\":'\n",
        "  end = ','\n",
        "  s = response_json\n",
        "  score = s.split(start)[1].split(end)[0].strip()\n",
        "\n",
        "  return int(score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A man is holding a briefcase and rushing to catch a train.\"\n",
        "print(\"--- \"+prompt+\" --- \")\n",
        "image_path = text_to_image(8888, prompt, 5)\n",
        "image = Image.open(image_path)\n",
        "image"
      ],
      "metadata": {
        "id": "aBPMdwMy22RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6581m7fNulj"
      },
      "outputs": [],
      "source": [
        "task=\"RLS-300\"\n",
        "file_name = \"/content/MLLM-Guidance/benchmarks/\" + task + \"_val.txt\"\n",
        "my_file = open(file_name, \"r\")\n",
        "\n",
        "data = my_file.read()\n",
        "\n",
        "captions = data.split(\"\\n\")\n",
        "if len(captions)>300:\n",
        "  captions = captions[:len(captions)-1]\n",
        "my_file.close()\n",
        "\n",
        "prompt = captions[0]\n",
        "print(\"--- \"+prompt+\" --- \")\n",
        "image_path = text_to_image(8888, prompt, 5)\n",
        "image = Image.open(image_path)\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gnxy-Y8r8shC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyODY/9s7NvJXc7odFtbVwxz"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}